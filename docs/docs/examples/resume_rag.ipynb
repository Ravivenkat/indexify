{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on Resume PDF files with Gemini's multimodal capabilities\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://getindexify.ai/\"><img src=\"https://getindexify.ai/Indexify_Logo_Wordmark.svg\" width=\"145\"></a>\n",
    "  <a href=\"https://discord.com/invite/kF8UZACA7r\"><img src=\"https://raw.githubusercontent.com/rishiraj/random/main/Discord%20button.png\" width=\"145\"></a><br>\n",
    "  Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/tensorlakeai/indexify\">Github</a></i> ⭐\n",
    "</div>\n",
    "\n",
    "### Step 1: Direct Data Extraction from PDF with Gemini\n",
    "\n",
    "The first step in Indexify's pipeline is to extract data, such as text, from various sources like PDF files. We understand that unstructured data poses a significant challenge and regular OCR based solutions can't always produce coherent & complete content. Hence, we use Gemini's multimodal capabilities to do the extraction.\n",
    "\n",
    "### Step 2: Enhanced Chunking with RecursiveCharacterTextSplitter\n",
    "\n",
    "Indexify's pipeline proceeds to perform chunking using the RecursiveCharacterTextSplitter algorithm. This algorithm has been specifically designed to handle large texts and create meaningful chunks based on a specified maximum chunk size.\n",
    "\n",
    "### Step 3: Embedding Creation with Snowflake's Arctic Model\n",
    "\n",
    "The final step in Indexify's pipeline is the creation of embeddings using Snowflake's Arctic embedding model. Embeddings are critical for enabling efficient similarity search and retrieval of relevant information from the chunked text.\n",
    "\n",
    "## Creating a PDF Extraction Pipeline is Simple with Indexify\n",
    "\n",
    "#### Install Indexify, Start the Server & Download the Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install indexify indexify-extractor-sdk\n",
    "\n",
    "# Download Indexify Server\n",
    "!curl https://getindexify.ai | sh\n",
    "\n",
    "# Download Extractors\n",
    "!indexify-extractor download hub://text/gemini\n",
    "!indexify-extractor download hub://text/chunking\n",
    "!indexify-extractor download hub://embedding/arctic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the necessary libraries, download the server, and the extractors, you need to restart the runtime. Then, you have to run Indexify Server with the Extractors.\n",
    "\n",
    "Open 2 terminals and run the following commands:\n",
    "\n",
    "```bash\n",
    "# Terminal 1\n",
    "./indexify server -d\n",
    "\n",
    "# Terminal 2\n",
    "indexify-extractor join-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Client, Define Extraction Graph & Ingest Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import ExtractionGraph\n",
    "\n",
    "extraction_graph_spec = \"\"\"\n",
    "name: 'geminiresume'\n",
    "extraction_policies:\n",
    "   - extractor: 'tensorlake/gemini'\n",
    "     name: 'pdfprocessor'\n",
    "     input_params:\n",
    "        model_name: 'gemini-1.5-flash-latest'\n",
    "        prompt: 'Extract all text from the document.'\n",
    "   - extractor: 'tensorlake/chunk-extractor'\n",
    "     name: 'chunker'\n",
    "     input_params:\n",
    "        chunk_size: 1000\n",
    "        overlap: 100\n",
    "     content_source: 'pdfprocessor'\n",
    "   - extractor: 'tensorlake/arctic'\n",
    "     name: 'embedder'\n",
    "     content_source: 'chunker'\n",
    "\"\"\"\n",
    "\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.get(\"https://www.overleaf.com/latex/templates/iit-dhanbad-resume-oncampus/sdtkcgtgxhtg.pdf\")\n",
    "\n",
    "with open('resume.pdf','wb') as f:\n",
    "    f.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29e347f7f00d02ad'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_id = client.upload_file(\"geminiresume\", \"resume.pdf\")\n",
    "client.wait_for_extraction(content_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing RAG with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question: str, index: str, top_k=2):\n",
    "    results = client.search_index(name=index, query=question, top_k=top_k)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        context = context + f\"content id: {result['content_id']} \\n\\n passage: {result['text']}\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the javascript related projects he has done?\"\n",
    "context = get_context(question, \"geminiresume.embedder.embedding\")\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, context):\n",
    "    return f\"Answer the question, based on the context.\\n question: {question} \\n context: {context}\"\n",
    "\n",
    "prompt = create_prompt(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client_openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client_openai.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorlake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
